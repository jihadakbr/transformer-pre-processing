
# Transformer Pre-processing

This project was completed as a part of the Honors portion of the [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) Course on [Coursera](https://www.coursera.org/).

Credit to DeepLearning.AI and the Coursera platform for providing the course materials and guidance.

## Objective

In this notebook, my objective is to explore and understand the pre-processing methods applied to raw text before passing it to the encoder and decoder blocks of the Transformer architecture.

Upon completing this assignment, I will gain the skills to create visualizations that provide insights into positional encodings, enhancing my understanding of how they impact word embeddings. By delving into the pre-processing techniques, I will be equipped with the knowledge to effectively prepare text data for the Transformer model, facilitating more accurate and meaningful representations.
## Results

![Transformer Pre-processing](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNjXhhEzcXdFMFoKqhF3cFp9XLsEnPuo9YfAWjJXRSXScfz_SemvrtbF0y1Xw5-pihB-fSd77wW4Q1_4jRO4CgBLUt8kLmIL7Op072buxS6w5Az7YXN6u8hxxCoso0bg9em75H3l7un48XaWBOr6k4QmofxMUDxuLospqFu2gQzAZuEb-GPPSpb-pFL0s/s1600/transformer-pre-processing.png)
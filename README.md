
# Transformer Pre-processing

This project was completed as a part of the Honors portion of the [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) Course on [Coursera](https://www.coursera.org/).

Credit to DeepLearning.AI and the Coursera platform for providing the course materials and guidance.

## Objective

In this notebook, my objective is to explore and understand the pre-processing methods applied to raw text before passing it to the encoder and decoder blocks of the Transformer architecture.

Upon completing this assignment, I will gain the skills to create visualizations that provide insights into positional encodings, enhancing my understanding of how they impact word embeddings. By delving into the pre-processing techniques, I will be equipped with the knowledge to effectively prepare text data for the Transformer model, facilitating more accurate and meaningful representations.
## Results

![Transformer Pre-processing](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYSAUqh--vp76dCnoS6lVqrOmGzDczGh3TH0pyYwxsAXrYZC6SRldzrYMJ90yaaa3lmCkwd392ou5ssaPlamiHzMA-JdTBON18VD6rxlK6vCCP0iO94hN2hECvzKi_J3wj7_hlUtOzYYLjJsVSxACT9ST00b7FxgJBtDwbGkHi-pw1u5lSt3IJItNiyME/s1600/transformer-pre-processing.png)